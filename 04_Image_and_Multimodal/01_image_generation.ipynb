{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Image Generation and Multimodal AI\n",
    "\n",
    "In this lab, you'll learn how to generate images using **Stable Diffusion** and understand images using **vision models** - all running locally.\n",
    "\n",
    "## What You'll Learn\n",
    "- Generate images from text prompts\n",
    "- Image-to-image transformations\n",
    "- Vision models for image understanding\n",
    "- Multimodal embeddings\n",
    "\n",
    "## Requirements\n",
    "- **GPU strongly recommended** for image generation\n",
    "- 8GB+ VRAM for Stable Diffusion\n",
    "- CPU works but is very slow (10+ minutes per image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate torch Pillow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Apple Silicon (MPS)\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU (this will be slow!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion XL Turbo (fast, good quality)\n",
    "# For older GPUs or less VRAM, use \"stabilityai/sd-turbo\" instead\n",
    "\n",
    "model_id = \"stabilityai/sdxl-turbo\"  # Fast, 4 steps\n",
    "# Alternative: \"runwayml/stable-diffusion-v1-5\"  # Classic, more compatible\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\"\n",
    "    )\n",
    "    pipe = pipe.to(device)\n",
    "else:\n",
    "    # For CPU/MPS\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    if device == \"mps\":\n",
    "        pipe = pipe.to(device)\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt: str, negative_prompt: str = None, steps: int = 4):\n",
    "    \"\"\"\n",
    "    Generate an image from a text prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: What you want to see\n",
    "        negative_prompt: What you don't want to see\n",
    "        steps: Number of diffusion steps (more = better quality, slower)\n",
    "    \"\"\"\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=0.0 if \"turbo\" in model_id else 7.5,  # Turbo doesn't need guidance\n",
    "    ).images[0]\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Display helper\n",
    "def show_image(image, title=\"Generated Image\"):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate your first image!\n",
    "prompt = \"A cozy coffee shop interior with warm lighting, plants, and wooden furniture\"\n",
    "\n",
    "print(f\"Generating: {prompt}\")\n",
    "image = generate_image(prompt)\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prompts\n",
    "prompts = [\n",
    "    \"A futuristic city skyline at sunset, cyberpunk style\",\n",
    "    \"A cute robot reading a book in a library\",\n",
    "    \"Abstract art with vibrant colors and geometric shapes\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nGenerating: {prompt}\")\n",
    "    image = generate_image(prompt)\n",
    "    show_image(image, prompt[:50] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering for Better Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good prompts include:\n",
    "# - Subject description\n",
    "# - Style/medium\n",
    "# - Lighting\n",
    "# - Quality tags\n",
    "\n",
    "detailed_prompt = \"\"\"\n",
    "A majestic mountain landscape at golden hour,\n",
    "snow-capped peaks reflecting sunset colors,\n",
    "crystal clear lake in foreground,\n",
    "professional photography, 8k, highly detailed,\n",
    "cinematic lighting, dramatic sky\n",
    "\"\"\".replace(\"\\n\", \" \")\n",
    "\n",
    "negative_prompt = \"blurry, low quality, distorted, ugly, bad anatomy\"\n",
    "\n",
    "image = generate_image(detailed_prompt, negative_prompt)\n",
    "show_image(image, \"Detailed Prompt Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style modifiers\n",
    "base_prompt = \"A cat sitting on a windowsill\"\n",
    "\n",
    "styles = [\n",
    "    \"oil painting style\",\n",
    "    \"anime style\",\n",
    "    \"pixel art style\",\n",
    "    \"watercolor style\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, style in zip(axes, styles):\n",
    "    prompt = f\"{base_prompt}, {style}\"\n",
    "    print(f\"Generating: {style}\")\n",
    "    image = generate_image(prompt)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(style)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./generated_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate and save\n",
    "prompt = \"A beautiful sunset over the ocean, dramatic clouds\"\n",
    "image = generate_image(prompt)\n",
    "\n",
    "# Save with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"{output_dir}/image_{timestamp}.png\"\n",
    "image.save(filename)\n",
    "\n",
    "print(f\"Saved to: {filename}\")\n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vision Models with Ollama\n",
    "\n",
    "Now let's use vision models to understand images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, pull a vision model\n",
    "# Run in terminal: ollama pull llava\n",
    "\n",
    "!pip install ollama -q\n",
    "import ollama\n",
    "import base64\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image: Image.Image) -> str:\n",
    "    \"\"\"Convert PIL Image to base64 string.\"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "def analyze_image(image: Image.Image, question: str = \"Describe this image in detail.\"):\n",
    "    \"\"\"Analyze an image using LLaVA vision model.\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='llava',\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': question,\n",
    "            'images': [image_to_base64(image)]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an image and then analyze it\n",
    "prompt = \"A robot chef cooking in a modern kitchen\"\n",
    "image = generate_image(prompt)\n",
    "show_image(image, \"Generated Image\")\n",
    "\n",
    "# Analyze it\n",
    "print(\"\\nVision model analysis:\")\n",
    "description = analyze_image(image)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask specific questions about images\n",
    "questions = [\n",
    "    \"What objects can you see in this image?\",\n",
    "    \"What is the mood or atmosphere of this image?\",\n",
    "    \"What artistic style does this image use?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    answer = analyze_image(image, question)\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze External Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze an image from file\n",
    "def analyze_image_file(filepath: str, question: str = \"Describe this image.\"):\n",
    "    \"\"\"Analyze an image file.\"\"\"\n",
    "    image = Image.open(filepath)\n",
    "    show_image(image, filepath)\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer = analyze_image(image, question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    return answer\n",
    "\n",
    "# Example: Analyze one of our saved images\n",
    "# analyze_image_file(\"./generated_images/your_image.png\")\n",
    "print(\"Uncomment above to analyze your own images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Image Captioning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image: Image.Image) -> str:\n",
    "    \"\"\"Generate a concise caption for an image.\"\"\"\n",
    "    prompt = \"Generate a short, descriptive caption for this image (1-2 sentences).\"\n",
    "    return analyze_image(image, prompt)\n",
    "\n",
    "def generate_tags(image: Image.Image) -> str:\n",
    "    \"\"\"Generate tags for an image.\"\"\"\n",
    "    prompt = \"List 5-10 relevant tags for this image, separated by commas.\"\n",
    "    return analyze_image(image, prompt)\n",
    "\n",
    "# Test on generated image\n",
    "prompt = \"A scientist in a lab coat examining colorful test tubes\"\n",
    "image = generate_image(prompt)\n",
    "show_image(image)\n",
    "\n",
    "print(\"Caption:\", generate_caption(image))\n",
    "print(\"\\nTags:\", generate_tags(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "- Generate images from text using Stable Diffusion\n",
    "- Write effective prompts for image generation\n",
    "- Apply different artistic styles\n",
    "- Use vision models (LLaVA) to analyze images\n",
    "- Build captioning and tagging pipelines\n",
    "\n",
    "**Key takeaways:**\n",
    "- Stable Diffusion runs completely locally\n",
    "- Prompt engineering greatly affects results\n",
    "- Vision models can understand and describe images\n",
    "- Combine generation + vision for powerful workflows\n",
    "\n",
    "**Next Lab:** Building AI Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
