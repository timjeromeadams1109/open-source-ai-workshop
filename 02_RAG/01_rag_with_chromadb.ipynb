{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "In this lab, you'll build a **RAG system** that can answer questions based on your own documents - completely locally and free.\n",
    "\n",
    "## What is RAG?\n",
    "RAG combines the power of LLMs with a knowledge base. Instead of relying only on what the model learned during training, RAG:\n",
    "1. **Retrieves** relevant documents from a database\n",
    "2. **Augments** the prompt with this context\n",
    "3. **Generates** an answer based on the retrieved information\n",
    "\n",
    "## What You'll Learn\n",
    "- Creating embeddings with local models\n",
    "- Storing vectors in ChromaDB\n",
    "- Semantic search and retrieval\n",
    "- Building a complete RAG pipeline\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install chromadb ollama sentence-transformers\n",
    "ollama pull llama3.2\n",
    "ollama pull nomic-embed-text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb ollama -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import ollama\n",
    "from typing import List\n",
    "\n",
    "print(\"ChromaDB version:\", chromadb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Embeddings\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar texts have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using Ollama\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding vector for text using Ollama.\"\"\"\n",
    "    response = ollama.embeddings(\n",
    "        model='nomic-embed-text',\n",
    "        prompt=text\n",
    "    )\n",
    "    return response['embedding']\n",
    "\n",
    "# Test it\n",
    "test_embedding = get_embedding(\"Hello, world!\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate semantic similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compare similar and different sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A feline rested on the rug\",  # Similar meaning\n",
    "    \"Python is a programming language\"  # Different topic\n",
    "]\n",
    "\n",
    "embeddings = [get_embedding(s) for s in sentences]\n",
    "\n",
    "print(\"Similarity scores:\")\n",
    "print(f\"'{sentences[0]}' vs '{sentences[1]}': {cosine_similarity(embeddings[0], embeddings[1]):.4f}\")\n",
    "print(f\"'{sentences[0]}' vs '{sentences[2]}': {cosine_similarity(embeddings[0], embeddings[2]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChromaDB client (in-memory for this demo)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# For persistent storage, use:\n",
    "# client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom embedding function for ChromaDB\n",
    "class OllamaEmbeddingFunction:\n",
    "    def __init__(self, model_name: str = \"nomic-embed-text\"):\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def __call__(self, input: List[str]) -> List[List[float]]:\n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            response = ollama.embeddings(\n",
    "                model=self.model_name,\n",
    "                prompt=text\n",
    "            )\n",
    "            embeddings.append(response['embedding'])\n",
    "        return embeddings\n",
    "\n",
    "# Create embedding function\n",
    "embed_fn = OllamaEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection\n",
    "collection = client.create_collection(\n",
    "    name=\"knowledge_base\",\n",
    "    embedding_function=embed_fn\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {collection.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding Documents to the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base about a fictional company\n",
    "documents = [\n",
    "    \"TechCorp was founded in 2020 by Jane Smith and John Doe in San Francisco.\",\n",
    "    \"TechCorp specializes in AI-powered productivity tools for remote teams.\",\n",
    "    \"The company's flagship product is TeamFlow, a project management platform.\",\n",
    "    \"TeamFlow uses machine learning to predict project timelines and identify bottlenecks.\",\n",
    "    \"TechCorp has 150 employees across offices in San Francisco, New York, and London.\",\n",
    "    \"The company raised $50 million in Series B funding in 2023.\",\n",
    "    \"TechCorp's annual revenue exceeded $20 million in 2023.\",\n",
    "    \"Jane Smith serves as CEO while John Doe is the CTO of TechCorp.\",\n",
    "    \"TeamFlow integrates with popular tools like Slack, GitHub, and Jira.\",\n",
    "    \"TechCorp offers a free tier for teams up to 10 members.\"\n",
    "]\n",
    "\n",
    "# Add documents to collection\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(f\"Added {len(documents)} documents to the collection\")\n",
    "print(f\"Collection count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "def search(query: str, n_results: int = 3):\n",
    "    \"\"\"Search the knowledge base for relevant documents.\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "query = \"Who founded the company?\"\n",
    "results = search(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    distance = results['distances'][0][i]\n",
    "    print(f\"  {i+1}. (distance: {distance:.4f}) {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different queries\n",
    "test_queries = [\n",
    "    \"How much funding did they raise?\",\n",
    "    \"What does the product do?\",\n",
    "    \"Where are the offices located?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search(query, n_results=2)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for doc in results['documents'][0]:\n",
    "        print(f\"  â†’ {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, n_context: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG:\n",
    "    1. Retrieve relevant documents\n",
    "    2. Build context from documents\n",
    "    3. Generate answer using LLM\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=n_context\n",
    "    )\n",
    "    \n",
    "    # Step 2: Build context\n",
    "    context = \"\\n\".join(results['documents'][0])\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    prompt = f\"\"\"Answer the question based only on the following context. If the answer is not in the context, say \"I don't have information about that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='llama3.2',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content'], results['documents'][0]\n",
    "\n",
    "# Test the RAG pipeline\n",
    "question = \"Who are the founders of TechCorp and what are their roles?\"\n",
    "answer, sources = rag_query(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\\n\")\n",
    "print(\"Sources used:\")\n",
    "for source in sources:\n",
    "    print(f\"  - {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with various questions\n",
    "questions = [\n",
    "    \"What is TeamFlow?\",\n",
    "    \"How many employees does TechCorp have?\",\n",
    "    \"What tools does TeamFlow integrate with?\",\n",
    "    \"What is TechCorp's revenue?\",\n",
    "    \"What is the weather like in San Francisco?\"  # Not in knowledge base\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer, _ = rag_query(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Document Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    Useful for long documents.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# Example with a longer document\n",
    "long_document = \"\"\"\n",
    "TechCorp's Journey: A Success Story\n",
    "\n",
    "TechCorp began as a small startup in a garage in San Francisco. Jane Smith, a former Google engineer, \n",
    "and John Doe, a Stanford PhD graduate, shared a vision of making remote work more efficient. They \n",
    "bootstrapped the company for the first year, building their MVP while working part-time jobs.\n",
    "\n",
    "The breakthrough came in early 2021 when they launched TeamFlow beta. Within six months, they had \n",
    "10,000 active users and caught the attention of venture capitalists. Their Series A round of $15 \n",
    "million allowed them to expand the team from 5 to 50 employees.\n",
    "\n",
    "By 2023, TeamFlow had become the go-to project management tool for tech startups. The platform's \n",
    "AI-powered features, including smart scheduling and resource allocation, set it apart from competitors. \n",
    "The Series B funding of $50 million valued the company at $200 million.\n",
    "\n",
    "Looking ahead, TechCorp plans to expand into enterprise markets and launch new products focused on \n",
    "AI-assisted coding and documentation. Jane Smith envisions a future where AI handles routine tasks, \n",
    "freeing humans to focus on creative and strategic work.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_text(long_document, chunk_size=300, overlap=30)\n",
    "print(f\"Split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk[:100] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Adding Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new collection with metadata\n",
    "docs_collection = client.create_collection(\n",
    "    name=\"docs_with_metadata\",\n",
    "    embedding_function=embed_fn\n",
    ")\n",
    "\n",
    "# Add documents with metadata\n",
    "docs_collection.add(\n",
    "    documents=[\n",
    "        \"TechCorp Q1 2023 revenue was $4.5 million\",\n",
    "        \"TechCorp Q2 2023 revenue was $5.2 million\",\n",
    "        \"TechCorp Q3 2023 revenue was $5.8 million\",\n",
    "        \"TechCorp Q4 2023 revenue was $6.1 million\",\n",
    "        \"Product roadmap includes AI assistant feature in Q1 2024\",\n",
    "        \"Product roadmap includes mobile app in Q2 2024\",\n",
    "    ],\n",
    "    ids=[\"q1_rev\", \"q2_rev\", \"q3_rev\", \"q4_rev\", \"roadmap_1\", \"roadmap_2\"],\n",
    "    metadatas=[\n",
    "        {\"type\": \"financial\", \"quarter\": \"Q1\", \"year\": 2023},\n",
    "        {\"type\": \"financial\", \"quarter\": \"Q2\", \"year\": 2023},\n",
    "        {\"type\": \"financial\", \"quarter\": \"Q3\", \"year\": 2023},\n",
    "        {\"type\": \"financial\", \"quarter\": \"Q4\", \"year\": 2023},\n",
    "        {\"type\": \"roadmap\", \"quarter\": \"Q1\", \"year\": 2024},\n",
    "        {\"type\": \"roadmap\", \"quarter\": \"Q2\", \"year\": 2024},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Added documents with metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with metadata filter\n",
    "results = docs_collection.query(\n",
    "    query_texts=[\"What was the revenue?\"],\n",
    "    n_results=5,\n",
    "    where={\"type\": \"financial\"}  # Only search financial documents\n",
    ")\n",
    "\n",
    "print(\"Financial documents only:\")\n",
    "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"  [{meta['quarter']} {meta['year']}] {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query roadmap documents\n",
    "results = docs_collection.query(\n",
    "    query_texts=[\"What features are planned?\"],\n",
    "    n_results=5,\n",
    "    where={\"type\": \"roadmap\"}\n",
    ")\n",
    "\n",
    "print(\"Roadmap documents only:\")\n",
    "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    print(f\"  [{meta['quarter']} {meta['year']}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For production use, persist the database to disk\n",
    "# Uncomment to use persistent storage:\n",
    "\n",
    "# persistent_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# \n",
    "# persistent_collection = persistent_client.get_or_create_collection(\n",
    "#     name=\"my_knowledge_base\",\n",
    "#     embedding_function=embed_fn\n",
    "# )\n",
    "# \n",
    "# # Add documents (they will persist across restarts)\n",
    "# persistent_collection.add(\n",
    "#     documents=[\"Your documents here\"],\n",
    "#     ids=[\"doc_1\"]\n",
    "# )\n",
    "\n",
    "print(\"See comments above for persistent storage example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "- Create embeddings using local models (Ollama)\n",
    "- Store and search vectors with ChromaDB\n",
    "- Understand semantic similarity\n",
    "- Build a complete RAG pipeline\n",
    "- Chunk long documents\n",
    "- Use metadata for filtered searches\n",
    "- Persist your vector database\n",
    "\n",
    "**Key takeaways:**\n",
    "- RAG allows LLMs to answer questions about your specific data\n",
    "- Embeddings capture semantic meaning, enabling similarity search\n",
    "- ChromaDB provides an easy-to-use vector database\n",
    "- Everything runs locally - no API costs!\n",
    "\n",
    "**Next Lab:** Model Customization with QLoRA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
