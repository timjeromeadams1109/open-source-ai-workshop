{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Text Generation with Local LLMs\n",
    "\n",
    "In this lab, you'll learn how to generate text using **Ollama**, a tool that lets you run large language models locally on your machine - completely free.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to interact with local LLMs using Ollama\n",
    "- Basic text generation and completion\n",
    "- Code generation\n",
    "- Structured output generation\n",
    "- Streaming responses\n",
    "\n",
    "## Prerequisites\n",
    "1. Install Ollama: `curl -fsSL https://ollama.com/install.sh | sh`\n",
    "2. Pull a model: `ollama pull llama3.2`\n",
    "3. Install Python package: `pip install ollama`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Basic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the ollama package if not already installed\n",
    "!pip install ollama -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Check available models\n",
    "models = ollama.list()\n",
    "print(\"Available models:\")\n",
    "for model in models['models']:\n",
    "    print(f\"  - {model['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text generation\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'What is machine learning? Explain in 2 sentences.'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different Generation Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Summarization\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural \n",
    "intelligence displayed by animals including humans. AI research has been defined as the field \n",
    "of study of intelligent agents, which refers to any system that perceives its environment and \n",
    "takes actions that maximize its chance of achieving its goals. The term \"artificial intelligence\" \n",
    "had previously been used to describe machines that mimic and display \"human\" cognitive skills \n",
    "that are associated with the human mind, such as \"learning\" and \"problem-solving\". This \n",
    "definition has since been rejected by major AI researchers who now describe AI in terms of \n",
    "rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f'Summarize this text in one sentence:\\n\\n{long_text}'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"Summary:\", response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering\n",
    "context = \"\"\"\n",
    "The Python programming language was created by Guido van Rossum and first released in 1991.\n",
    "Python's design philosophy emphasizes code readability with the use of significant indentation.\n",
    "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms,\n",
    "including structured, object-oriented and functional programming.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who created Python and when was it released?\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f'Based on this context:\\n{context}\\n\\nAnswer this question: {question}'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"Answer:\", response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Python code\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Write a Python function that checks if a number is prime. Include docstring and type hints.'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code explanation\n",
    "code_to_explain = \"\"\"\n",
    "def fibonacci(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f'Explain what this Python code does line by line:\\n{code_to_explain}'\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming allows you to see the response as it's generated\n",
    "print(\"Streaming response:\")\n",
    "stream = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Write a short poem about programming.'\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "print()  # newline at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. System Prompts and Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using system prompts to control behavior\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a helpful coding assistant. Always provide code examples and explain your reasoning step by step.'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'How do I read a CSV file in Python?'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a specialized assistant\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a SQL expert. Respond only with SQL queries, no explanations unless asked.'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Get all users who signed up in the last 30 days from a users table'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conversation History (Multi-turn Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'My name is Alex and I\\'m learning Python.'},\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=messages)\n",
    "print(\"Assistant:\", response['message']['content'])\n",
    "\n",
    "# Add assistant's response to history\n",
    "messages.append(response['message'])\n",
    "\n",
    "# Continue the conversation\n",
    "messages.append({'role': 'user', 'content': 'What\\'s my name and what am I learning?'})\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=messages)\n",
    "print(\"\\nAssistant:\", response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting temperature for creativity\n",
    "prompt = \"Write a creative name for a coffee shop\"\n",
    "\n",
    "# Low temperature = more deterministic\n",
    "response_low = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    options={'temperature': 0.1}\n",
    ")\n",
    "print(\"Low temperature (0.1):\", response_low['message']['content'])\n",
    "\n",
    "# High temperature = more creative/random\n",
    "response_high = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    options={'temperature': 1.5}\n",
    ")\n",
    "print(\"High temperature (1.5):\", response_high['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Request structured JSON output\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': '''Extract information from this text and return as JSON:\n",
    "        \"John Smith is a 35-year-old software engineer from San Francisco who works at TechCorp.\"\n",
    "        \n",
    "        Return JSON with fields: name, age, profession, city, company'''\n",
    "    }],\n",
    "    format='json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['message']['content'])\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Try Different Models\n",
    "\n",
    "Ollama supports many open-source models. Here are some you can try:\n",
    "\n",
    "| Model | Size | Best For |\n",
    "|-------|------|----------|\n",
    "| llama3.2 | 3B | General purpose, fast |\n",
    "| llama3.1 | 8B | Better quality, slower |\n",
    "| mistral | 7B | Good balance of speed/quality |\n",
    "| codellama | 7B | Code generation |\n",
    "| phi3 | 3.8B | Fast, good for simple tasks |\n",
    "\n",
    "Pull a new model with: `ollama pull <model-name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models on the same task\n",
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "for model_name in ['llama3.2']:  # Add more models you've pulled\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(response['message']['content'])\n",
    "    except Exception as e:\n",
    "        print(f\"{model_name}: Not available - run 'ollama pull {model_name}' first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to:\n",
    "- Use Ollama to run LLMs locally\n",
    "- Generate text, summaries, and answers\n",
    "- Generate and explain code\n",
    "- Stream responses in real-time\n",
    "- Use system prompts to control behavior\n",
    "- Maintain conversation history\n",
    "- Adjust model parameters like temperature\n",
    "- Get structured JSON output\n",
    "\n",
    "**Next Lab:** Building a RAG system with ChromaDB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
